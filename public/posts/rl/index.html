<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reinforcement Learning Notes | Zhiyang&#39;s Blog</title>
<meta name="keywords" content="note">
<meta name="description" content="Notes on everything related with RL.">
<meta name="author" content="Zhiyang Shen">
<link rel="canonical" href="https://shenzhiy21.github.io/posts/rl/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d63079815d18f277504cf66aa2f64064cea74ac90c714e35aa4680da8d8b05c4.css" integrity="sha256-1jB5gV0Y8ndQTPZqovZAZM6nSskMcU41qkaA2o2LBcQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shenzhiy21.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shenzhiy21.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shenzhiy21.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shenzhiy21.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://shenzhiy21.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shenzhiy21.github.io/posts/rl/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


<meta property="og:title" content="Reinforcement Learning Notes" />
<meta property="og:description" content="Notes on everything related with RL." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shenzhiy21.github.io/posts/rl/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-09-20T18:21:52+08:00" />
<meta property="article:modified_time" content="2025-09-20T18:21:52+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Reinforcement Learning Notes"/>
<meta name="twitter:description" content="Notes on everything related with RL."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://shenzhiy21.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reinforcement Learning Notes",
      "item": "https://shenzhiy21.github.io/posts/rl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reinforcement Learning Notes",
  "name": "Reinforcement Learning Notes",
  "description": "Notes on everything related with RL.",
  "keywords": [
    "note"
  ],
  "articleBody": "Sutton Book Notes Chapter 3: Finite Markov Decision Processes Problem Formulation: “an agent learns from interaction (with environment) to achieve a goal”.\nAt each time step $t$:\nAgent gets a state $S_t$ from environment. Agent selects an action $A_t$ based on $S_t$. Agent receives a reward $R_{t+1}$ from environment, and finds itself in a new state $S_{t+1}$. At each time step $t$, the agent implements a mapping from states to probabilities of selecting each possible action, called policy, denoted $\\pi_t$. $\\pi_t(a|s)$ is the probability that $A_t = a$ if $S_t = s$.\nThe agent’s goal is to maximize the cumulative reward it receives over the long run, often called return: $$ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$ where $0\\leq \\gamma\\leq 1$ is called the discount rate.\nMarkov property: if for all $s’$, $r$, and histories $S_0, A_0, R_1, \\cdots, S_{t-1}, A_{t-1}, R_t, S_t, A_t$, we have $$ \\begin{aligned} \u0026\\text{Pr}\\{R_{t+1}=r,S_{t+1}=s’ |S_t,A_t \\}\\\\ ={}\u0026\\text{Pr}\\{R_{t+1}=r,S_{t+1}=s’|S_0,A_0,R_1,\\cdots,S_{t-1},A_{t-1},R_t,S_t,A_t \\} \\end{aligned} $$\nIn this book, assume every environment to have Markov property. Many real-world scenarios can be viewed as an approximate Markov. Also, this requires the “state” to be informative enough to encode all the information needed for state transition.\nA reinforcement learning task that satisfies the Markov property is called a Markov decision process, or MDP. If the state and action spaces are finite, it’s called a finite MDP.\nA finite MDP is defined by the one-step dynamic of the environment: $$ p(s’,r|s,a)=\\text{Pr}\\{S_{t+1}=s’,R_{t+1}=r|S_t=s,A_t=a \\} $$\nThen we can compute the expected rewards for state-action pairs: $$ r(s,a) =\\mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\\sum_{r\\in\\mathcal R}r\\sum_{s’\\in\\mathcal S}p(s’,r|s,a) $$\nThe state-transition probabilities: $$ p(s’|s,a)=\\text{Pr}\\{S_{t+1}=s’|S_t=s,A_t=a \\}=\\sum_{r\\in\\mathcal R}p(s’,r|s,a) $$\nThe expected rewards for state-action-next_state triples: $$ r(s,a,s’)=\\mathbb E\\left[R_{t+1}\\middle|S_t=s,A_t=a,S_{t+1}=s’\\right]=\\frac{\\sum_{r\\in\\mathcal R}rp(s’,r|s,a)}{p(s’|s,a)} $$\nValue function: functions of states or state-action pairs that estimates how good it is for that state (or state-action pair). Formally, the state-value function of a state $s$ under a policy $\\pi$, denoted $v_{\\pi}(s)$, is the expected return when starting in $s$ and following $\\pi$ thereafter. $$ v_{\\pi}(s) = \\mathbb E_{\\pi}\\left[G_t\\middle|S_t=s \\right]=\\mathbb E_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\middle|S_t=s \\right] $$\nSimilarly, the action-value function of taking action $a$ in state $s$ under policy $\\pi$ is defined as: $$ q_{\\pi}(s, a) = \\mathbb E_{\\pi}\\left[G_t\\middle|S_t=s,A_t=a \\right]=\\mathbb E_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} \\middle| S_t=s, A_t=a \\right] $$\nThese functions can be estimated by many methods, e.g.:\nMonte Carlo methods parameterized function approximators Bellman equation:\n$$ \\begin{aligned} v_{\\pi}(s) \u0026= \\mathbb E_{\\pi}\\left[G_t \\middle| S_t=s\\right]\\\\ \u0026= \\mathbb E_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\middle|S_t=s \\right]\\\\ \u0026= \\mathbb E_{\\pi}\\left[R_{t+1}+\\gamma\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+2}\\middle|S_t=s \\right]\\\\ \u0026= \\sum_{a} \\pi(a|s)\\sum_{s’}\\sum_{r}p(s’,r|s,a)\\left[r+\\gamma\\mathbb E_{\\pi}\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+2}\\middle|S_{t+1}=s’ \\right] \\right]\\\\ \u0026= \\sum_a \\pi(a|s)\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma v_{\\pi}(s’) \\right] \\end{aligned} $$\nIn some cases, the probability of (starting from state $s$, and taking action $a$ to move to state $s’$) doesn’t depend on reward $r$, i.e., this state transition always outputs the same reward. Therefore, the reward will be a function of $s$ and $a$, denoted as $R_s^a$. Then, we don’t have to take expectation for variable $r$: $$ v_{\\pi}(s)=\\sum_a\\pi(a|s)\\sum_{s’}p(s’|s,a)[R_s^a+\\gamma v_\\pi(s’)] $$\nThis kind of MDP can be induced to a Markov Reward Process (MRP) according to the policy $\\pi$, defined as: $$ \\mathcal P_{s,s’}^\\pi =\\sum_{a}\\pi(a|s)p(s’|s,a), \\ \\ R_s^\\pi = \\sum_a \\pi(a|s)R_s^a $$\nAnd the Bellman equation can be expressed by the induced MRP, in a vectorized form: $$ \\vec{v}_{\\pi} = \\vec{R}^{\\pi} + \\gamma \\mathcal{P}^{\\pi}\\vec{v}_{\\pi} $$\nOptimal policy: for any MDP, there exists an optimal policy $\\pi_*$ that is “better than” all other policies: $$ v_{\\pi_*}(s)\\geq v_{\\pi}(s),\\forall \\pi,\\forall s $$\nThis also leads to the optimal state-value function and optimal action-value function, denoted $v_*(s)$ and $q_*(s,a)$ respectively.\nWe have Bellman optimality equation for the optimal state-value function: $$ \\begin{aligned} v_{*}(s) \u0026= \\max_a q_{*}(s,a) \\\\ \u0026= \\max_a \\mathbb E_{\\pi_*}\\left[G_t\\middle| S_t=s,A_t=a \\right] \\\\ \u0026= \\max_a \\mathbb E_{\\pi_*}\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\middle |S_t=s,A_t=a \\right]\\\\ \u0026= \\max_a \\mathbb E_{\\pi_*}\\left[R_{t+1} + \\gamma\\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} \\middle | S_t=s,A_t=a \\right] \\\\ \u0026= \\max_a \\mathbb E_{\\pi_*}\\left[R_{t+1}+\\gamma v_{*}(S_{t+1})\\middle| S_t=s,A_t=a \\right] \\\\ \u0026= \\max_a\\sum_{s’,r}p(s’,r|s,a)[r+\\gamma v_* (s’)] \\end{aligned} $$\nAnd also the Bellman optimality equation for $q_*$: $$ \\begin{aligned} q_*(s,a) \u0026= \\mathbb E_{\\pi_*} \\left[R_{t+1}+\\gamma\\max_{a’}q_*(S_{t+1},a’)\\middle| S_t=s,A_t=a \\right]\\\\ \u0026= \\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma\\max_{a’}q_*(s’,a’) \\right] \\end{aligned} $$\nBellman optimality equations only hold true for the optimal policy!\nTraditionally, a RL algorithm learns either $v_*$ or $q_*$. Now the question is, how to decide the optimal policy?\nIf it learns $q_*(s,a)$, then it only needs to choose the action that maximizes $q_*(s,a)$ for the current state $s$: $$ \\pi^*(s)= \\argmax_a q_*(s,a) $$\nIf it learns the optimal state-value function $v_*(s)$, then it’s more difficult. Consider two cases:\nThe environment is known, i.e., we know the state transition $p(s’,r|s,a)$. Therefore, we can first get $q_*(s,a)$ by Bellman optimality equation, and take $\\argmax$. The environment is unknown. Here, we must approximate the environment by sampling. Otherwise, consider to learn $q_*$ instead. However, modern RL algorithms often learn the parameterized policy $\\pi_\\theta(a|s)$ directly, which is more robust, more suitable for continuous action spaces, and easier to implement under deep learning frameworks.\nSome classic RL algorithms:\nAlgorithm Environment What it Learns Common Methods Dynamic Programming (DP) Known $v(s)$ - Monte Carlo (MC) Known $q(s,a)$ MCTS Temporal-Difference (TD) Unknown $q(s,a)$ SARSA, Q-Learning Policy Gradient Unknown $π(a|s)$ REINFORCE, Actor-Critic (e.g. PPO) ",
  "wordCount" : "825",
  "inLanguage": "en",
  "datePublished": "2025-09-20T18:21:52+08:00",
  "dateModified": "2025-09-20T18:21:52+08:00",
  "author":{
    "@type": "Person",
    "name": "Zhiyang Shen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shenzhiy21.github.io/posts/rl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Zhiyang's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shenzhiy21.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shenzhiy21.github.io/" accesskey="h" title="Zhiyang&#39;s Blog (Alt + H)">Zhiyang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shenzhiy21.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://shenzhiy21.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/shenzhiy21" title="Github">
                    <span>Github</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Reinforcement Learning Notes
    </h1>
    <div class="post-meta"><span title='2025-09-20 18:21:52 +0800 CST'>September 20, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Zhiyang Shen

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#sutton-book-notes" aria-label="Sutton Book Notes">Sutton Book Notes</a><ul>
                        
                <li>
                    <a href="#chapter-3-finite-markov-decision-processes" aria-label="Chapter 3: Finite Markov Decision Processes">Chapter 3: Finite Markov Decision Processes</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="sutton-book-notes">Sutton Book Notes<a hidden class="anchor" aria-hidden="true" href="#sutton-book-notes">#</a></h2>
<h3 id="chapter-3-finite-markov-decision-processes">Chapter 3: Finite Markov Decision Processes<a hidden class="anchor" aria-hidden="true" href="#chapter-3-finite-markov-decision-processes">#</a></h3>
<p>Problem Formulation: &ldquo;an <em>agent</em> learns from interaction (with <em>environment</em>) to achieve a goal&rdquo;.</p>
<p><img loading="lazy" src="/images/rl/pipeline.png" alt=""  />
</p>
<p>At each time step $t$:</p>
<ul>
<li>Agent gets a state $S_t$ from environment.</li>
<li>Agent selects an action $A_t$ based on $S_t$.</li>
<li>Agent receives a reward $R_{t+1}$ from environment, and finds itself in a new state $S_{t+1}$.</li>
</ul>
<p>At each time step $t$, the agent implements a mapping from states to probabilities of selecting each possible action, called <em>policy</em>, denoted $\pi_t$. $\pi_t(a|s)$ is the probability that $A_t = a$ if $S_t = s$.</p>
<p>The agent&rsquo;s goal is to maximize the <em>cumulative reward</em> it receives over the long run, often called <em>return</em>:
$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
where $0\leq \gamma\leq 1$ is called the <em>discount rate</em>.</p>
<p>Markov property: if for all $s&rsquo;$, $r$, and histories $S_0, A_0, R_1, \cdots, S_{t-1}, A_{t-1}, R_t, S_t, A_t$, we have
$$
\begin{aligned}
&amp;\text{Pr}\{R_{t+1}=r,S_{t+1}=s&rsquo; |S_t,A_t \}\\
={}&amp;\text{Pr}\{R_{t+1}=r,S_{t+1}=s&rsquo;|S_0,A_0,R_1,\cdots,S_{t-1},A_{t-1},R_t,S_t,A_t \}
\end{aligned}
$$</p>
<p>In this book, assume every environment to have Markov property. Many real-world scenarios can be viewed as an approximate Markov.
Also, this requires the &ldquo;state&rdquo; to be <em>informative</em> enough to encode all the information needed for state transition.</p>
<p>A reinforcement learning task that satisfies the Markov property is called a <em>Markov decision process</em>, or <em>MDP</em>. If the state and action spaces are finite, it&rsquo;s called a <em>finite MDP</em>.</p>
<p>A finite MDP is defined by the one-step dynamic of the environment:
$$
p(s&rsquo;,r|s,a)=\text{Pr}\{S_{t+1}=s&rsquo;,R_{t+1}=r|S_t=s,A_t=a \}
$$</p>
<p>Then we can compute the expected rewards for state-action pairs:
$$
r(s,a) =\mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\sum_{r\in\mathcal R}r\sum_{s&rsquo;\in\mathcal S}p(s&rsquo;,r|s,a)
$$</p>
<p>The state-transition probabilities:
$$
p(s&rsquo;|s,a)=\text{Pr}\{S_{t+1}=s&rsquo;|S_t=s,A_t=a \}=\sum_{r\in\mathcal R}p(s&rsquo;,r|s,a)
$$</p>
<p>The expected rewards for state-action-next_state triples:
$$
r(s,a,s&rsquo;)=\mathbb E\left[R_{t+1}\middle|S_t=s,A_t=a,S_{t+1}=s&rsquo;\right]=\frac{\sum_{r\in\mathcal R}rp(s&rsquo;,r|s,a)}{p(s&rsquo;|s,a)}
$$</p>
<p><em>Value function</em>: functions of <em>states</em> or <em>state-action pairs</em> that estimates how good it is for that state (or state-action pair).
Formally, the <em>state-value function</em> of a state $s$ under a policy $\pi$, denoted $v_{\pi}(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter.
$$
v_{\pi}(s) = \mathbb E_{\pi}\left[G_t\middle|S_t=s \right]=\mathbb E_{\pi}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\middle|S_t=s \right]
$$</p>
<p>Similarly, the <em>action-value function</em> of taking action $a$ in state $s$ under policy $\pi$ is defined as:
$$
q_{\pi}(s, a) = \mathbb E_{\pi}\left[G_t\middle|S_t=s,A_t=a \right]=\mathbb E_{\pi}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \middle| S_t=s, A_t=a \right]
$$</p>
<p>These functions can be estimated by many methods, <em>e.g.</em>:</p>
<ul>
<li>Monte Carlo methods</li>
<li>parameterized function approximators</li>
</ul>
<p><em>Bellman equation</em>:</p>
<p>$$
\begin{aligned}
v_{\pi}(s) &amp;= \mathbb E_{\pi}\left[G_t \middle| S_t=s\right]\\
&amp;= \mathbb E_{\pi}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\middle|S_t=s \right]\\
&amp;= \mathbb E_{\pi}\left[R_{t+1}+\gamma\sum_{k=0}^{\infty}\gamma^k R_{t+k+2}\middle|S_t=s \right]\\
&amp;= \sum_{a} \pi(a|s)\sum_{s&rsquo;}\sum_{r}p(s&rsquo;,r|s,a)\left[r+\gamma\mathbb E_{\pi}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+2}\middle|S_{t+1}=s&rsquo; \right] \right]\\
&amp;= \sum_a \pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma v_{\pi}(s&rsquo;) \right]
\end{aligned}
$$</p>
<p>In some cases, the probability of (starting from state $s$, and taking action $a$ to move to state $s&rsquo;$) doesn&rsquo;t depend on reward $r$, <em>i.e.</em>, this state transition always outputs the same reward.
Therefore, the reward will be a function of $s$ and $a$, denoted as $R_s^a$.
Then, we don&rsquo;t have to take expectation for variable $r$:
$$
v_{\pi}(s)=\sum_a\pi(a|s)\sum_{s&rsquo;}p(s&rsquo;|s,a)[R_s^a+\gamma v_\pi(s&rsquo;)]
$$</p>
<p>This kind of MDP can be <em>induced</em> to a <em>Markov Reward Process</em> (MRP) according to the policy $\pi$, defined as:
$$
\mathcal P_{s,s&rsquo;}^\pi =\sum_{a}\pi(a|s)p(s&rsquo;|s,a), \ \  R_s^\pi = \sum_a \pi(a|s)R_s^a
$$</p>
<p>And the Bellman equation can be expressed by the induced MRP, in a vectorized form:
$$
\vec{v}_{\pi} = \vec{R}^{\pi} + \gamma \mathcal{P}^{\pi}\vec{v}_{\pi}
$$</p>
<p><em>Optimal policy</em>: for any MDP, there exists an optimal policy $\pi_*$ that is &ldquo;better than&rdquo; all other policies:
$$
v_{\pi_*}(s)\geq v_{\pi}(s),\forall \pi,\forall s
$$</p>
<p>This also leads to the <em>optimal state-value function</em> and <em>optimal action-value function</em>, denoted $v_*(s)$ and $q_*(s,a)$ respectively.</p>
<p>We have <em>Bellman optimality equation</em> for the optimal state-value function:
$$
\begin{aligned}
v_{*}(s) &amp;= \max_a q_{*}(s,a) \\
&amp;= \max_a \mathbb E_{\pi_*}\left[G_t\middle| S_t=s,A_t=a \right] \\
&amp;= \max_a \mathbb E_{\pi_*}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1}\middle |S_t=s,A_t=a \right]\\
&amp;= \max_a \mathbb E_{\pi_*}\left[R_{t+1} + \gamma\sum_{k=0}^\infty \gamma^k R_{t+k+2} \middle | S_t=s,A_t=a \right] \\
&amp;= \max_a \mathbb E_{\pi_*}\left[R_{t+1}+\gamma v_{*}(S_{t+1})\middle| S_t=s,A_t=a \right] \\
&amp;= \max_a\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)[r+\gamma v_* (s&rsquo;)]
\end{aligned}
$$</p>
<p>And also the <em>Bellman optimality equation</em> for $q_*$:
$$
\begin{aligned}
q_*(s,a) &amp;= \mathbb E_{\pi_*} \left[R_{t+1}+\gamma\max_{a&rsquo;}q_*(S_{t+1},a&rsquo;)\middle| S_t=s,A_t=a \right]\\
&amp;= \sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma\max_{a&rsquo;}q_*(s&rsquo;,a&rsquo;) \right]
\end{aligned}
$$</p>
<blockquote>
<p>Bellman optimality equations only hold true for the optimal policy!</p></blockquote>
<p>Traditionally, a RL algorithm learns either $v_*$ or $q_*$. Now the question is, how to decide the optimal policy?</p>
<p>If it learns $q_*(s,a)$, then it only needs to choose the action that maximizes $q_*(s,a)$ for the current state $s$:
$$
\pi^*(s)= \argmax_a q_*(s,a)
$$</p>
<p>If it learns the optimal state-value function $v_*(s)$, then it&rsquo;s more difficult. Consider two cases:</p>
<ol>
<li>The environment is known, <em>i.e.</em>, we know the state transition $p(s&rsquo;,r|s,a)$. Therefore, we can first get $q_*(s,a)$ by Bellman optimality equation, and take $\argmax$.</li>
<li>The environment is unknown. Here, we must approximate the environment by sampling. Otherwise, consider to learn $q_*$ instead.</li>
</ol>
<p>However, modern RL algorithms often learn the parameterized policy $\pi_\theta(a|s)$ directly, which is more robust, more suitable for continuous action spaces, and easier to implement under deep learning frameworks.</p>
<p>Some classic RL algorithms:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Algorithm</th>
          <th style="text-align: center">Environment</th>
          <th style="text-align: center">What it Learns</th>
          <th style="text-align: center">Common Methods</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">Dynamic Programming (DP)</td>
          <td style="text-align: center">Known</td>
          <td style="text-align: center">$v(s)$</td>
          <td style="text-align: center">-</td>
      </tr>
      <tr>
          <td style="text-align: center">Monte Carlo (MC)</td>
          <td style="text-align: center">Known</td>
          <td style="text-align: center">$q(s,a)$</td>
          <td style="text-align: center">MCTS</td>
      </tr>
      <tr>
          <td style="text-align: center">Temporal-Difference (TD)</td>
          <td style="text-align: center">Unknown</td>
          <td style="text-align: center">$q(s,a)$</td>
          <td style="text-align: center">SARSA, Q-Learning</td>
      </tr>
      <tr>
          <td style="text-align: center">Policy Gradient</td>
          <td style="text-align: center">Unknown</td>
          <td style="text-align: center">$π(a|s)$</td>
          <td style="text-align: center">REINFORCE, Actor-Critic (e.g. PPO)</td>
      </tr>
  </tbody>
</table>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shenzhiy21.github.io/tags/note/">Note</a></li>
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning Notes on x"
            href="https://x.com/intent/tweet/?text=Reinforcement%20Learning%20Notes&amp;url=https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f&amp;hashtags=note">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning Notes on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f&amp;title=Reinforcement%20Learning%20Notes&amp;summary=Reinforcement%20Learning%20Notes&amp;source=https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning Notes on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f&title=Reinforcement%20Learning%20Notes">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning Notes on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning Notes on whatsapp"
            href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%20Notes%20-%20https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning Notes on telegram"
            href="https://telegram.me/share/url?text=Reinforcement%20Learning%20Notes&amp;url=https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Reinforcement Learning Notes on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Reinforcement%20Learning%20Notes&u=https%3a%2f%2fshenzhiy21.github.io%2fposts%2frl%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>@ 2025 Zhiyang Shen</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
